name: CI/CD Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        node-version: [18.x]
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3

    # Backend Build
    - name: Setup Node.js (Backend)
      uses: actions/setup-node@v3
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'
        cache-dependency-path: backend/package-lock.json
    - name: Install Backend Dependencies
      run: |
        cd backend
        npm ci

    # Frontend Build
    - name: Setup Node.js (Frontend)
      uses: actions/setup-node@v3
      with:
        node-version: ${{ matrix.node-version }}
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
    - name: Install Frontend Dependencies
      run: |
        cd frontend
        npm ci
    - name: Build Frontend
      run: |
        cd frontend
        npm run build

  docker-build-and-push:
    needs: build-and-test
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Log in to the Container registry
        uses: docker/login-action@v2
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Lowercase Repo Owner
        run: |
          echo "REPO_OWNER=$(echo ${{ github.repository_owner }} | tr '[:upper:]' '[:lower:]')" >> $GITHUB_ENV

      - name: Build and Push Backend
        uses: docker/build-push-action@v4
        with:
          context: ./backend
          push: true
          tags: ghcr.io/${{ env.REPO_OWNER }}/fooddelivery-backend:latest



  deploy:
    needs: docker-build-and-push
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Setup SSH
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.EC2_SSH_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -H ${{ secrets.EC2_HOST }} >> ~/.ssh/known_hosts

      - name: Copy docker-compose to EC2
        run: |
          ssh -i ~/.ssh/id_rsa ${{ secrets.EC2_USER }}@${{ secrets.EC2_HOST }} "mkdir -p /home/${{ secrets.EC2_USER }}/app"
          scp -i ~/.ssh/id_rsa docker-compose.yml ${{ secrets.EC2_USER }}@${{ secrets.EC2_HOST }}:/home/${{ secrets.EC2_USER }}/app/docker-compose.yml

      - name: Deploy to EC2
        run: |
          ssh -i ~/.ssh/id_rsa ${{ secrets.EC2_USER }}@${{ secrets.EC2_HOST }} << 'EOF'
            # Detect OS and install Docker
            if command -v apt-get &> /dev/null; then
               # Ubuntu/Debian
               if ! command -v docker &> /dev/null; then
                   echo "Installing Docker (apt)..."
                   sudo apt-get update
                   sudo apt-get install -y docker.io
                   sudo usermod -aG docker ${{ secrets.EC2_USER }}
               fi
               if ! command -v docker-compose &> /dev/null; then
                   sudo apt-get install -y docker-compose
               fi
            elif command -v yum &> /dev/null; then
               # Amazon Linux / CentOS
               if ! command -v docker &> /dev/null; then
                   echo "Installing Docker (yum)..."
                   sudo yum update -y
                   sudo yum install -y docker
                   sudo service docker start
                   sudo usermod -aG docker ${{ secrets.EC2_USER }}
               fi
               # Install docker-compose via binary for Amazon Linux
               if ! command -v docker-compose &> /dev/null; then
                   echo "Installing Docker Compose (binary)..."
                   sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
                   sudo chmod +x /usr/local/bin/docker-compose
                   # Ensure it's in path
                   if ! command -v docker-compose &> /dev/null; then
                       sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
                   fi
               fi
            else
               echo "Unsupported Package Manager. Please install Docker manually."
               exit 1
            fi

            mkdir -p /home/${{ secrets.EC2_USER }}/app
            cd /home/${{ secrets.EC2_USER }}/app

            # Configure Nginx Client Max Body Size (Fix for 413 Payload Too Large)
            if command -v nginx &> /dev/null; then
               echo "Configuring Nginx limit..."
               # Create a dedicated config file for limits if it doesn't exist
               echo "client_max_body_size 50M;" | sudo tee /etc/nginx/conf.d/upload_limit.conf
               # Reload Nginx to apply changes
               sudo service nginx reload || sudo systemctl reload nginx
               echo "Nginx reloaded."
            fi
            
            # Create/Update .env file with secrets
            echo "NODE_ENV=production" > .env
            
            # Update source code for manual dev server usage
            # Check if git exists and repo is initialized
            if [ -d .git ]; then
                echo "Updating source code..."
                git fetch origin main || true # Ignore fetch errors if HEAD detached
                git reset --hard origin/main || true
            else
                echo "Cloning repository..."
                # Remove current directory contents to clone fresh (careful with .env which we just made)
                # Actually, better to just init and pull
                git init
                git remote add origin https://github.com/${{ github.repository }}.git
                git fetch origin main
                git reset --hard origin/main
            fi

            # Re-create .env after git operations (just in case git reset wiped it)
            echo "NODE_ENV=production" > .env
            echo "MONGO_URI=${{ secrets.MONGO_URI }}" >> .env
            echo "JWT_SECRET=${{ secrets.JWT_SECRET }}" >> .env
            echo "JWT_EXPIRE=30d" >> .env
            echo "FRONTEND_URL=${{ secrets.FRONTEND_URL }}" >> .env
            echo "EMAIL_SERVICE=${{ secrets.EMAIL_SERVICE }}" >> .env
            echo "EMAIL_USER=${{ secrets.EMAIL_USER }}" >> .env
            echo "EMAIL_PASS=${{ secrets.EMAIL_PASS }}" >> .env
            # AWS Configuration
            echo "AWS_REGION=${{ secrets.AWS_REGION }}" >> .env
            echo "AWS_BUCKET_NAME=${{ secrets.AWS_BUCKET_NAME }}" >> .env
            echo "AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}" >> .env
            echo "AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}" >> .env

            # Login to GHCR on EC2 (as root/sudo to ensure docker access)
            echo ${{ secrets.GITHUB_TOKEN }} | sudo docker login ghcr.io -u ${{ github.actor }} --password-stdin

            # Pull latest images explicitly
            sudo docker-compose pull

            # Restart containers with force-recreate to ensure new image is used
            sudo docker-compose up -d --force-recreate --remove-orphans
            
            # Prune unused images to save space
            sudo docker image prune -f
          EOF

  deploy-frontend-s3:
    needs: build-and-test
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18.x'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install Dependencies
        run: |
          cd frontend
          npm ci

      - name: Build Frontend
        env:
          VITE_API_URL: /api
        run: |
          cd frontend
          npm run build

      - name: Setup SSH
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.EC2_SSH_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -H ${{ secrets.EC2_HOST }} >> ~/.ssh/known_hosts

      - name: Upload frontend build to EC2
        run: |
          # Create target directory first
          ssh -i ~/.ssh/id_rsa ${{ secrets.EC2_USER }}@${{ secrets.EC2_HOST }} "mkdir -p /home/${{ secrets.EC2_USER }}/frontend-deploy-temp"
          # Recursive copy
          scp -i ~/.ssh/id_rsa -r frontend/dist ${{ secrets.EC2_USER }}@${{ secrets.EC2_HOST }}:/home/${{ secrets.EC2_USER }}/frontend-deploy-temp/dist

      - name: Deploy to S3 via EC2
        run: |
          ssh -i ~/.ssh/id_rsa ${{ secrets.EC2_USER }}@${{ secrets.EC2_HOST }} << 'EOF'
            # Ensure AWS CLI is installed
            if ! command -v aws &> /dev/null; then
              sudo yum install -y aws-cli || sudo apt-get install -y awscli
            fi

            # Sync to S3 using EC2's IAM Role
            # Note: scp -r created /home/user/frontend-deploy-temp/dist
            
            echo "=== DEBUG: Listing deploy directory ==="
            ls -R /home/${{ secrets.EC2_USER }}/frontend-deploy-temp
            echo "======================================="

            # Check if dist exists directly or if it's nested
            aws s3 sync /home/${{ secrets.EC2_USER }}/frontend-deploy-temp/dist s3://${{ secrets.AWS_FRONTEND_BUCKET_NAME }}

            # Invalidate CloudFront Cache (if Distribution ID is provided)
            # We use a conditional check to avoid failure if the secret is missing
            if [ ! -z "${{ secrets.CLOUDFRONT_DISTRIBUTION_ID }}" ]; then
                echo "Invalidating CloudFront Cache..."
                aws cloudfront create-invalidation --distribution-id ${{ secrets.CLOUDFRONT_DISTRIBUTION_ID }} --paths "/*"
            else
                echo "Skipping CloudFront Invalidation (Secret CLOUDFRONT_DISTRIBUTION_ID not set)"
            fi

            # Cleanup
            rm -rf /home/${{ secrets.EC2_USER }}/frontend-deploy-temp
          EOF
